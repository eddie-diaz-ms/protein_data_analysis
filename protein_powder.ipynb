{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Three â€” Predicting a Target Market for Protein Powders\n",
    "### by Eddie Diaz\n",
    "\n",
    "Dataset Selected: https://dsld.od.nih.gov/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Overview\n",
    "\n",
    "For this lab, I'll be using the DSLD government database, which contains every supplement sold in the US. For simplicity, I will only download the portion of data for our target subgroup, protein powders sold in the US. Using this subset, I will train a model to predict which target demographic best suits a particular product based on serving size, daily value, and key ingredient. This mix of quantitative and categorical values will enable us to best predict the target demographic. To do this, I will first define a dummy categorical variable based on serving size. These values will be one-hot encoded to designate their respective target demographic in ascending order (low, mid high). The logic behind this process was explained in Lab One. Then, I'll use a predictive linear regression algorithm to obtain weight values for each feature to predict a multi-class target value that nears our \"product type,\" the dummy variable.\n",
    "\n",
    "This prediction task will enable us to know which target demographic best suits a product based on given data. For example,  we cam know which products better suit athletes, people on a specific diet, and other buyer personas that can be commercialized into a marketplace platform. By offering the best supplement products to each respective individual, we can not only produce more sales, but we can also ensure long-lasting satisfaction with each purchase, avoid health issues like toxicity, and improve defective industry practices like oversized servings. Because supplement recommendations affect people's health, this algorithm should perform exceedingly well for deployment. Nevertheless, right now we just want to better understand the market for machine learning exploration.\n",
    "\n",
    "Once again, I know that most data in this csv is not useful. Many attributes seem to have the wrong label, and features seem to be mixed out of place. For example, I doubt \"Adults and children 4 or more years of age\" is actually supposed to be under the ingredients column. Luckily, my background in Nutrition Coaching allows me to impute the correct label for each attribute, so we can build a small workable set of data. \n",
    "\n",
    "Items are repeated often which can alter our results, so I'll have to create a filtering algorithm to select only one of each item. Unfortunately, this also means that the actual ingredient variable (listed under DSLD Ingredient Categories) is useless. Individual items are repeated for each ingredient, so if a powder contains Vitamin A and Protein, it would be listed twice. In addition, many ingredients are listed with faulty data, as \"Calories\" is not an actual ingredient. For these reasons, I decided to drop the ingredient variable altogether; it would be too complicated to clean for this project. \n",
    "\n",
    "To further simplify our task, I will have to delete all items not in grams, as measurements such as \"2 Scoops\" are difficult to standardize. And finally, I will ensure all items without complete data are also removed for simplicity. In a commercialization endeavor, this would also benefit our business, as misleading or incomplete labels are unsafe to recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   serving_size daily_value\n",
      "0          98.0         120\n",
      "1          96.0          80\n",
      "2          96.0          78\n",
      "3          96.0          78\n",
      "4          96.0          78\n",
      "serving_size    2070\n",
      "daily_value     1565\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Original code written by me. Entries not in grams were deleted manually using Microsoft Excel.\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "supplements = []\n",
    "\n",
    "with open(\"./report/DietarySupplementFacts.csv\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "\n",
    "    for row in reader:\n",
    "        if row[\"DSLD Ingredient Categories\"] == \"Protein\":\n",
    "            serving = row[\"Serving Size\"]\n",
    "            grams = float(serving.rstrip(\"gGram(s)\"))\n",
    "            powder = {\n",
    "                \"serving_size\" : grams,\n",
    "                \"daily_value\": row[\"Daily Value Target Group\"]\n",
    "            }\n",
    "            supplements.append(powder)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "csv_header = [\"serving_size\", \"daily_value\"]\n",
    "proteins = \"proteins.csv\"\n",
    "with open(proteins, \"w\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(supplements)\n",
    "\n",
    "protein_df = pd.read_csv(proteins)\n",
    "\n",
    "print(protein_df.head())\n",
    "print(protein_df.count())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a small set, but this data seems way more workable. \n",
    "\n",
    "I quickly noticed that for 2070 data points,, only 1565 contain %DV. Although we could impute values for each of these data points, the lack of ingredient data makes it difficult to accurately do so. As such, we will delete items without a %DV. Additionally, following the outlier logic described in lab one, items over 60g will be removed from the list. These values are oversized and unadvisable to recommend, as excess protein is simply treated as waste by the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    serving_size daily_value\n",
      "19           8.5          16\n",
      "20           8.0          12\n",
      "21           8.0          12\n",
      "68           7.2           5\n",
      "70           7.0          12\n",
      "serving_size    1409\n",
      "daily_value     1409\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "protein_df.drop(protein_df[(protein_df['serving_size'] >= 60)].index, inplace = True)\n",
    "protein_df.dropna(inplace=True)\n",
    "print(protein_df.head())\n",
    "print(protein_df.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. We have a dataset with 1409 items in a standardized format. It's only missing the dummy variable and the target colum, which I'll add below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    serving_size daily_value  product_type\n",
      "19           8.5          16             0\n",
      "20           8.0          12             0\n",
      "21           8.0          12             0\n",
      "68           7.2           5             0\n",
      "70           7.0          12             0\n"
     ]
    }
   ],
   "source": [
    "protein_df[\"product_type\"] = protein_df[\"serving_size\"].apply(lambda x: 0 if x < 20 else (2 if x >= 45 else 1))\n",
    "print(protein_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_df.daily_value = pd.to_numeric(protein_df.daily_value, 'coerce')\n",
    "protein_df.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the code above after realizing that daily value was actually a string. \n",
    "Now for some summary statistics on our numeric data categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "serving_size    32.0\n",
       "daily_value     48.0\n",
       "product_type     1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_df[[\"serving_size\", \"daily_value\", \"product_type\"]].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serving_size</th>\n",
       "      <th>daily_value</th>\n",
       "      <th>product_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1405.000000</td>\n",
       "      <td>1405.000000</td>\n",
       "      <td>1405.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.822206</td>\n",
       "      <td>44.418833</td>\n",
       "      <td>0.971530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.702390</td>\n",
       "      <td>11.425083</td>\n",
       "      <td>0.338487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.400000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       serving_size  daily_value  product_type\n",
       "count   1405.000000  1405.000000   1405.000000\n",
       "mean      30.822206    44.418833      0.971530\n",
       "std        8.702390    11.425083      0.338487\n",
       "min        1.000000     1.000000      0.000000\n",
       "25%       28.400000    40.000000      1.000000\n",
       "50%       32.000000    48.000000      1.000000\n",
       "75%       35.000000    50.000000      1.000000\n",
       "max       58.000000   100.000000      2.000000"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_df[[\"serving_size\", \"daily_value\", \"product_type\"]].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Looking good, but now I need to implement a 80/20 split.\n",
    "\n",
    "I believe this split is a little skewed towards the training set. Having only 20% of all supplement data in the US leaves very little room for multiple iterations of the model. In this case, the model may \"overfit\" the training set. Therefore, the end prediction will be less polished. However, this large training set is also very helpful, as the machine will have a large sample to better predict new product's end market. It's a double-edged sword. If this model were to be deployed, I would probably change the split to allow for more training, but it will suffice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = protein_df[\"product_type\"].to_numpy() # define the target variable (dependent variable) as y\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    serving_size  daily_value\n",
      "19             8           16\n",
      "20             8           12\n",
      "21             8           12\n",
      "68             7            5\n",
      "70             7           12\n"
     ]
    }
   ],
   "source": [
    "protein_df['serving_size'] = protein_df['serving_size'].astype('int64')\n",
    "protein_df['daily_value'] = protein_df['daily_value'].astype('int64')\n",
    "protein_df = protein_df.drop([\"product_type\"], axis=1)\n",
    "\n",
    "print(protein_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1124, 2) (1124,)\n",
      "(281, 2) (281,)\n"
     ]
    }
   ],
   "source": [
    "# create training and testing X and Y split\n",
    "# based on this source https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(protein_df, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're ready to create the model.\n",
    "\n",
    "## Modeling\n",
    "\n",
    "First, we'll create a custom model that uses steepest ascent method for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Object with coefficients:\n",
      "[[-0.13238434]\n",
      " [ 8.39104576]\n",
      " [ 5.57756722]]\n",
      "Accuracy of:  0.896797153024911\n"
     ]
    }
   ],
   "source": [
    "# Based on Dr. Larson's code, https://github.com/eclarson/MachineLearningNotebooks/blob/master/06.%20Optimization.ipynb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "\n",
    "class ClassifyLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add because maximizing \n",
    "\n",
    "blr = ClassifyLogisticRegression(eta=0.1,iterations=50,C=0.001)\n",
    "\n",
    "blr.fit(X_train,y_train)\n",
    "print(blr)\n",
    "\n",
    "yhat = blr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Object with coefficients:\n",
      "[[  2.88976272]\n",
      " [190.06827055]\n",
      " [217.25796278]]\n",
      "Accuracy of:  0.896797153024911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/j5z935293gb554y0mbshh4n00000gn/T/ipykernel_51140/2960004216.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(ma.log(g[y==1]))-ma.sum(np.log(1-g[y==0])) + C*sum(wnew**2)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_optimize.py:2233: RuntimeWarning: invalid value encountered in subtract\n",
      "  r = (xf - nfc) * (fx - ffulc)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_optimize.py:2234: RuntimeWarning: invalid value encountered in subtract\n",
      "  q = (xf - fulc) * (fx - fnfc)\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "import copy\n",
    "from numpy import ma # (masked array) this has most numpy functions that work with NaN data.\n",
    "class LineSearchLogisticRegression(ClassifyLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    def __init__(self, line_iters=0.0, **kwds):        \n",
    "        self.line_iters = line_iters\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "    \n",
    "    # this defines the function with the first input to be optimized\n",
    "    # therefore eta will be optimized, with all inputs constant\n",
    "    @staticmethod\n",
    "    def objective_function(eta,X,y,w,grad,C):\n",
    "        wnew = w - grad*eta\n",
    "        g = expit(X @ wnew)\n",
    "        # the line search is looking for minimization, so take the negative of l(w)\n",
    "        return -np.sum(ma.log(g[y==1]))-ma.sum(np.log(1-g[y==0])) + C*sum(wnew**2)\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = -self._get_gradient(Xb,y)\n",
    "            # minimization is in opposite direction\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.line_iters} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.objective_function, # objective function to optimize\n",
    "                                  bounds=(0,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ -= gradient*eta # set new function values\n",
    "            # subtract to minimize\n",
    "                \n",
    "            \n",
    "\n",
    "lslr = LineSearchLogisticRegression(eta=1,\n",
    "                                    iterations=5, \n",
    "                                    line_iters=5, \n",
    "                                    C=0.001)\n",
    "\n",
    "lslr.fit(X_train,y_train)\n",
    "\n",
    "yhat = lslr.predict(X_test)\n",
    "print(lslr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model seems to be accurate at predicting the target market, I believe it is bound to be inaccurate because the data itself is faulty. In addition, the target variable is directly based on the serving size, making our model like a self-fulfilling prophecy. Furthermore, we can see that our optimization technique was unsuccessful, as we obtained the same accuracy score. Again, I truly believe this is a problem with my data source. Hopefully a visualization from sklearn will elucidate our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94090112, 1.14362572, 1.14362572, 1.14362572, 0.84588308])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.6374347750772795\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "print(\"Score\", model.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data is accurately being clustered into three different subgroups, one for true supplements \"0\", another for meal replacements \"1\", and one for increased metabolic demand \"2\". However, we can also see that our custom model performed better, as it has a superior accuracy score by almost 30%. Nevertheless, the model is still lacking due to poor data input.\n",
    "\n",
    "However, we can still test out for learning purposes how other optimizations improve the model. Let's try BGFS Hessian approximations, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 1.39794311 -0.43094634  0.12513884]\n",
      " [-1.42571699  0.00381683  0.08001956]\n",
      " [-2.43482383  0.22350073 -0.2091847 ]]\n",
      "Accuracy of:  0.9359430604982206\n"
     ]
    }
   ],
   "source": [
    "# also based on Dr. Larson's notebook on Optimization\n",
    "from scipy.optimize import fmin_bfgs # maybe the most common bfgs algorithm in the world\n",
    "from numpy import ma\n",
    "class BFGSLogisticRegression(ClassifyLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(w**2) \n",
    "        #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "            \n",
    "bfgslr = BFGSLogisticRegression(_,iterations=2,C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, \n",
    "                 C=0.0001, \n",
    "                 solver=BFGSLogisticRegression):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            \n",
    "            hblr = self.solver(eta=self.eta,iterations=self.iters,C=self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "lr = MultiClassLogisticRegression(eta=1,\n",
    "    iterations=10,\n",
    "    C=0.01,\n",
    "    solver=BFGSLogisticRegression\n",
    "    )\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That actually gave us some really good accuracy! The model seems like it's working better thanks to the complex mathematics introduced by Quasi-Newton optimization. However, I still do not feel comfortable with the results, as I understand they are based on poor data. Nevertheless, we can now see that long-handed approaches with complex optimizations work better than generic libraries like SciPy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "After developing two custom model and comparing it to sklearn, it is unadvisable to deploy either. Despite the higher performance of the custom model, the data is incomplete, muddled, and needs extensive cleaning. Furthermore, out target is directly based on our quantitative values, and the output would simply be a matter of encoding. Better data would be required to deploy a commercially viable model that could actually recommend products to people. As it stands, models based on this data are doomed to fail and should not be used predict an actual person's health needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
